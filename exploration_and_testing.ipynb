{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stateful Neuron Exploration\n",
    "\n",
    "This notebook will test the performance of the stateful neural network PyTorch port, to verify that the method is \n",
    "implemented correctly and behaves as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# General imports\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import copy\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Imports for the tokenizer, the dataset, and the model\n",
    "from transformers import GPT2Tokenizer\n",
    "from utils.datasets import TextDataLoader\n",
    "from models.transformer_model import TransformerModel\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# MNIST dataset hyperparameters\n",
    "MNIST_INPUT_SIZE = 784\n",
    "MNIST_LAYER_SIZES = [MNIST_INPUT_SIZE, 128, 64, 10]\n",
    "MNIST_BATCH_SIZE = 1000\n",
    "\n",
    "# Sunspot dataset hyperparameters\n",
    "SUNSPOT_INPUT_SIZE = 12\n",
    "SUNSPOT_LAYER_SIZES = [SUNSPOT_INPUT_SIZE, 128, 64, 1]\n",
    "SUNSPOT_BATCH_SIZE = 100\n",
    "\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "# Device configuration\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Set notebook to reload external python modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 1: Stateful Neuron vs. FCN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Dataset Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training set and loader\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "trainset = datasets.MNIST(root='./data/test/', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=MNIST_BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Create test set and loader\n",
    "testset = datasets.MNIST(root='./data/test/', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=MNIST_BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function: Train Network on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_data, test_data, criterion, opt, epochs):\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Training phase\n",
    "        model.train()  # Set the model to training mode\n",
    "        train_loss = 0.0\n",
    "        for inputs, labels in tqdm(train_data, desc=f'Training: Epoch {epoch+1}/{epochs}', unit='batch'):\n",
    "            # Move the data to the device\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Flatten the images\n",
    "            inputs = inputs.view(inputs.shape[0], -1)  # Flatten the images\n",
    "\n",
    "            # Zero the gradients\n",
    "            opt.zero_grad()\n",
    "\n",
    "            # Forward pass and loss calculation\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and weight update\n",
    "            loss.backward(retain_graph=True)\n",
    "            opt.step()\n",
    "\n",
    "            # Logging the loss\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Testing phase\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        test_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_data:\n",
    "                # Move the data to the device\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Flatten the images\n",
    "                inputs = inputs.view(inputs.shape[0], -1)\n",
    "\n",
    "                # Forward pass and loss calculation\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Logging the loss and updating variables at batch level\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Logging the losses\n",
    "        train_loss /= len(trainloader)\n",
    "        test_loss /= len(testloader)\n",
    "        test_accuracy = 100 * correct / total\n",
    "        print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `StatefulNeuronNetwork`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neurons(nn.Module):\n",
    "    def __init__(self, n_neurons, device=\"cuda\"):\n",
    "        super(Neurons, self).__init__()\n",
    "        self.n_neurons = n_neurons\n",
    "\n",
    "        self.ones = torch.ones(n_neurons, 1, requires_grad=False).to(device)\n",
    "        self.hidden = nn.Parameter(torch.zeros(n_neurons, 1), requires_grad=False).to(device)\n",
    "        self.params = nn.Parameter(torch.rand(n_neurons, 3, 3) * 2 - 1).to(device)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Expand self.ones and self.hidden to match the batch size\n",
    "        ones_expanded = self.ones.expand(inputs.shape[0], -1, -1)\n",
    "        hidden_expanded = self.hidden.expand(inputs.shape[0], -1, -1)\n",
    "\n",
    "        inputs_unsqueezed = inputs.unsqueeze(-1) \n",
    "\n",
    "        # Concatenate along the second dimension (dim=1)\n",
    "        stacked = torch.cat((inputs_unsqueezed, ones_expanded, hidden_expanded), dim=1)  \n",
    "\n",
    "        \n",
    "        final_shape = stacked.view(inputs.shape[0], -1, 3).unsqueeze(-1)\n",
    "\n",
    "        # Compute dot product and apply tanh activation\n",
    "        dot = torch.tanh(torch.matmul(self.params, final_shape).squeeze())\n",
    "\n",
    "        # Update hidden state and clone x\n",
    "        x = dot[:, :, 0].clone()\n",
    "       \n",
    "        # Compute the mean across the batch dimension (dim=0)\n",
    "        mean_hidden = torch.mean(dot[:, :, -1], dim=0)\n",
    "\n",
    "        # Reshape to get [self.n_neurons, 1]\n",
    "        new_hidden = mean_hidden.view(self.n_neurons, 1)\n",
    "\n",
    "        return x, dot, new_hidden\n",
    "\n",
    "class NeuralDiverseNet(nn.Module):\n",
    "    def __init__(self, sizes):\n",
    "        super(NeuralDiverseNet, self).__init__()\n",
    "        self.neurons = nn.ModuleList([Neurons(size) for size in sizes])\n",
    "        # Using nn.Linear layers instead of manual weight parameters\n",
    "        self.linear_layers = nn.ModuleList([nn.Linear(sizes[i], sizes[i + 1]) for i in range(len(sizes) - 1)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        #pre, pre_dot = self.neurons[0].neuron_fn(x)\n",
    "\n",
    "        for i, (neuron, linear_layer) in enumerate(zip(self.neurons[1:], self.linear_layers)):\n",
    "            x = linear_layer(x)  # Using the nn.Linear layer here\n",
    "            x, _, new_hidden = neuron(x)\n",
    "\n",
    "            neuron.hidden = nn.Parameter(new_hidden, requires_grad=False)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0346613618d648b2ad167d3b7e2585c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: Epoch 1/5:   0%|          | 0/60 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 2.0876, Test Loss: 1.9075, Test Accuracy: 48.57%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13d292e6201a499bba39d04a7b6c177c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: Epoch 2/5:   0%|          | 0/60 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Loss: 1.8635, Test Loss: 1.8128, Test Accuracy: 47.05%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f5cfab60c1d483abbbe113b41ddac14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: Epoch 3/5:   0%|          | 0/60 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train Loss: 1.8061, Test Loss: 1.7808, Test Accuracy: 47.31%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f2acedc0ee74a678c18318c4842051a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: Epoch 4/5:   0%|          | 0/60 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train Loss: 1.7804, Test Loss: 1.7641, Test Accuracy: 47.79%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6870efb9cd9e4c3fb40af8882ad73e3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: Epoch 5/5:   0%|          | 0/60 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Train Loss: 1.7657, Test Loss: 1.7524, Test Accuracy: 48.09%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d660d09231941939e21f104f45a333d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: Epoch 6/10:   0%|          | 0/60 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Train Loss: 1.7555, Test Loss: 1.7470, Test Accuracy: 48.25%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c55fc11eaea4835813ebdb94942f417",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: Epoch 7/10:   0%|          | 0/60 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Train Loss: 1.7483, Test Loss: 1.7398, Test Accuracy: 48.61%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e85753a004044a9a30577ed3f588746",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: Epoch 8/10:   0%|          | 0/60 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Train Loss: 1.7425, Test Loss: 1.7372, Test Accuracy: 48.62%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed3fa5b17b1742019833e53fa079ff08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: Epoch 9/10:   0%|          | 0/60 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Train Loss: 1.7391, Test Loss: 1.7335, Test Accuracy: 48.76%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70812ea2394c45798475cfe08fc05d26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: Epoch 10/10:   0%|          | 0/60 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Train Loss: 1.7344, Test Loss: 1.7315, Test Accuracy: 48.66%\n"
     ]
    }
   ],
   "source": [
    "stateful_neuron_model = NeuralDiverseNet(MNIST_LAYER_SIZES).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(stateful_neuron_model.parameters(), lr=0.001)\n",
    "\n",
    "train(model=stateful_neuron_model, device=DEVICE, train_data=trainloader, test_data=testloader, \n",
    "      criterion=criterion, opt=optimizer, epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `FeedforwardNetwork`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, MNIST_LAYER_SIZES):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(len(MNIST_LAYER_SIZES) - 1):\n",
    "            self.layers.append(nn.Linear(MNIST_LAYER_SIZES[i], MNIST_LAYER_SIZES[i + 1]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = torch.relu(layer(x))\n",
    "        x = self.layers[-1](x)  # No activation after the last layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6ef3dd67b7d403dbdb5db02005e50a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: Epoch 1/5:   0%|          | 0/60 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.9722, Test Loss: 0.4075, Test Accuracy: 88.50%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02a8f944f8d341c589125aa0e35df6bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: Epoch 2/5:   0%|          | 0/60 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Loss: 0.3653, Test Loss: 0.3129, Test Accuracy: 90.74%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad5788d86da64fb8ba82c7086e1ab08e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: Epoch 3/5:   0%|          | 0/60 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train Loss: 0.3053, Test Loss: 0.2760, Test Accuracy: 91.97%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "558edf3b4b3646e298c2504c2ba1217e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: Epoch 4/5:   0%|          | 0/60 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train Loss: 0.2691, Test Loss: 0.2475, Test Accuracy: 92.67%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35e9608dad28458ba2254007ede5cea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: Epoch 5/5:   0%|          | 0/60 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Train Loss: 0.2407, Test Loss: 0.2190, Test Accuracy: 93.44%\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "ff_model = FeedForwardNetwork(MNIST_LAYER_SIZES).to(DEVICE)\n",
    "ff_optimizer = torch.optim.Adam(ff_model.parameters(), lr=0.001)\n",
    "ff_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "train(model=ff_model, device=DEVICE, train_data=trainloader, test_data=testloader, \n",
    "      criterion=ff_criterion, opt=ff_optimizer, epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `SpikingNeuralNetwork`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def surrogate_gradient(x):\n",
    "    alpha = 10  # The steepness of the surrogate gradient\n",
    "    return torch.sigmoid(alpha * x)\n",
    "\n",
    "class SpikingNeuronLayer(nn.Module):\n",
    "    def __init__(self, size_in, size_out, device):\n",
    "        super(SpikingNeuronLayer, self).__init__()\n",
    "        self.device = device\n",
    "        self.synaptic_weights = nn.Parameter(torch.randn(size_in, size_out, device=device) * 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(self.device)\n",
    "        pre_synaptic = torch.matmul(x, self.synaptic_weights)\n",
    "        post_synaptic = surrogate_gradient(pre_synaptic - 1)\n",
    "        return post_synaptic\n",
    "\n",
    "class SpikingNeuralNetwork(nn.Module):\n",
    "    def __init__(self, MNIST_LAYER_SIZES, device):\n",
    "        super(SpikingNeuralNetwork, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.device = device\n",
    "        for i in range(len(MNIST_LAYER_SIZES) - 1):\n",
    "            self.layers.append(SpikingNeuronLayer(MNIST_LAYER_SIZES[i], MNIST_LAYER_SIZES[i + 1], device))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(self.device)  # Ensure input tensor is on the correct device\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fd857676a844b8b941cbf5de8859408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: Epoch 1/5:   0%|          | 0/60 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 2.3026, Test Loss: 2.3026, Test Accuracy: 11.35%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0b2e0051ee9487987520e15be69c63d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: Epoch 2/5:   0%|          | 0/60 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Loss: 2.3026, Test Loss: 2.3026, Test Accuracy: 11.35%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "644b3ef360ba487f96fd5c5b53a66656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: Epoch 3/5:   0%|          | 0/60 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train Loss: 2.3026, Test Loss: 2.3026, Test Accuracy: 11.35%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c8f61f33f38425285e36a0bfa8c95db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: Epoch 4/5:   0%|          | 0/60 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train Loss: 2.3026, Test Loss: 2.3026, Test Accuracy: 11.35%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "258acc5e83f84c5e9a04069e8bcc9c86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: Epoch 5/5:   0%|          | 0/60 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Train Loss: 2.3026, Test Loss: 2.3026, Test Accuracy: 11.35%\n"
     ]
    }
   ],
   "source": [
    "# Create the network\n",
    "snn_model = SpikingNeuralNetwork([784, 128, 64, 10], device=DEVICE).to(DEVICE)\n",
    "snn_optimizer = torch.optim.Adam(snn_model.parameters(), lr=0.001)\n",
    "snn_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "train(model=snn_model, device=DEVICE, train_data=trainloader, test_data=testloader, \n",
    "      criterion=snn_criterion, opt=snn_optimizer, epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 2: Stateful Neurons vs. RNN\n",
    "Do stateful neurons perform similarly to recurrent neural networks on a simple time series task?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length: 2400\n",
      "Testing set length: 800\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('data/test/Sunspots.csv', usecols=['Monthly Mean Total Sunspot Number'])\n",
    "data = df.values.astype(float)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data_normalized = scaler.fit_transform(data)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "data_normalized = torch.FloatTensor(data_normalized).view(-1)\n",
    "\n",
    "# Create sequences and corresponding labels\n",
    "sequence_length = 12  # For example, use 12 months to predict the next month\n",
    "sequences = []\n",
    "labels = []\n",
    "\n",
    "for i in range(len(data_normalized) - sequence_length):\n",
    "    sequences.append(data_normalized[i:i+sequence_length])\n",
    "    labels.append(data_normalized[i+sequence_length])\n",
    "\n",
    "sequences = torch.stack(sequences[:-1])\n",
    "labels = torch.stack(labels[1:])\n",
    "\n",
    "# Split the data into training, validation, and testing sets\n",
    "train_sequences, test_sequences, train_labels, test_labels = train_test_split(\n",
    "    sequences, labels, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "# Trim the dataset to an easily divisible length\n",
    "train_sequences = train_sequences[:2400]\n",
    "train_labels = train_labels[:2400]\n",
    "test_sequences = test_sequences[:800]\n",
    "test_labels = test_labels[:800]\n",
    "\n",
    "# Create DataLoaders for each set\n",
    "sunspot_train_loader = DataLoader(TensorDataset(train_sequences, train_labels), shuffle=True, \n",
    "                                  batch_size=SUNSPOT_BATCH_SIZE)\n",
    "sunspot_test_loader = DataLoader(TensorDataset(test_sequences, test_labels), shuffle=False, \n",
    "                                 batch_size=SUNSPOT_BATCH_SIZE)\n",
    "\n",
    "# Print the length of each set\n",
    "print(f'Training set length: {len(train_sequences)}')\n",
    "print(f'Testing set length: {len(test_sequences)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function: Train Network on Sunspot Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_time_series(model, train_loader, val_loader, criterion, opt, epochs, device=DEVICE, model_type=None):\n",
    "    # Perform training\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for sequences, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}', unit='batch'):\n",
    "\n",
    "            # Move the data to the device and reshape the targets\n",
    "            sequences, labels = sequences.to(device), labels.to(device)\n",
    "            sequences = sequences.view(sequences.shape[0], SUNSPOT_INPUT_SIZE, 1).to(device)\n",
    "            labels = labels.unsqueeze(1).to(device)  # Reshape targets\n",
    "\n",
    "            # Zero the gradients\n",
    "            opt.zero_grad()\n",
    "\n",
    "            # Forward pass and loss calculation\n",
    "            outputs = model(sequences)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and weight update\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            # Logging the loss\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for sequences, labels in val_loader:\n",
    "                # Move the data to the device and reshape the targets\n",
    "                sequences, labels = sequences.to(device), labels.to(device)\n",
    "                sequences = sequences.view(sequences.shape[0], SUNSPOT_INPUT_SIZE, 1).to(device)\n",
    "                labels = labels.unsqueeze(1).to(device)  # Reshape targets\n",
    "\n",
    "                # Forward pass and loss calculation\n",
    "                outputs = model(sequences)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f\"Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `RecurrentNeuralNetwork`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaRNN(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=12, output_size=1):\n",
    "        super(VanillaRNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.fc(out[:, -1, :])  # Using the last time step's output\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42ad4611e6f04a32989456c4b4d7616d",
       "model_id": "91957948804042a3b080ca2a0e111126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/10:   0%|          | 0/24 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.4698, Validation Loss: 0.4294\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e2e066d26b24bb0b2335abf8e3b1965",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/10:   0%|          | 0/24 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.3908, Validation Loss: 0.3567\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83a0681aefbc45c1bd9f9e9ce4fd8fe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/10:   0%|          | 0/24 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.3244, Validation Loss: 0.2966\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4cabaed65014aa2b3923378cfa5cff6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/10:   0%|          | 0/24 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.2702, Validation Loss: 0.2478\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "877bc0cd0156420cbe273078476fb261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/10:   0%|          | 0/24 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.2260, Validation Loss: 0.2083\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c85a15f75044c618adba8dcd5eddc70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/5:   0%|          | 0/24 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6743, Validation Loss: 0.6094\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c37ba17b9054c95b53d548f697a6c53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/5:   0%|          | 0/24 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5567, Validation Loss: 0.5024\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d22d293f9d24a55a4abd3b9ba7eec53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/5:   0%|          | 0/24 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.4586, Validation Loss: 0.4129\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84c4a847e2e54524b1ea51c9f652a635",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/5:   0%|          | 0/24 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.3774, Validation Loss: 0.3406\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f7a68e47e6441bdb1c0db5d76adb83e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/5:   0%|          | 0/24 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.3114, Validation Loss: 0.2816\n"
     ]
    }
   ],
   "source": [
    "rnn_model = VanillaRNN(input_size=1, hidden_size=1, output_size=1).to(DEVICE)\n",
    "rnn_criterion = nn.MSELoss()\n",
    "rnn_optimizer = torch.optim.Adam(rnn_model.parameters(), lr=0.001)\n",
    "\n",
    "train_time_series(model=rnn_model, model_type='RNN', device=DEVICE, train_loader=sunspot_train_loader, \n",
    "      val_loader=sunspot_test_loader, criterion=rnn_criterion, opt=rnn_optimizer, epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `RecurrentStatefulNeuron`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialNeuralDiverseNet(nn.Module):\n",
    "    def __init__(self, sizes):\n",
    "        super(SequentialNeuralDiverseNet, self).__init__()\n",
    "        self.neurons = nn.ModuleList([Neurons(size) for size in sizes])\n",
    "        self.linear_layers = nn.ModuleList([nn.Linear(sizes[i], sizes[i + 1]) for i in range(len(sizes) - 1)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.squeeze()\n",
    "        for i, (neuron, linear_layer) in enumerate(zip(self.neurons[1:], self.linear_layers)):\n",
    "            x = linear_layer(x)  # Using the nn.Linear layer here\n",
    "            x, _, new_hidden = neuron(x)\n",
    "\n",
    "            neuron.hidden = nn.Parameter(new_hidden, requires_grad=False)\n",
    "\n",
    "        final_output = final_output.view(batch_size, -1)\n",
    "\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03202352d3844fb4849269bad072c945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/5:   0%|          | 0/24 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [

     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 492.00 MiB (GPU 0; 10.00 GiB total capacity; 16.10 GiB already allocated; 0 bytes free; 16.13 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/nburley/Recurrent-Neuron-Transformer/exploration_and_testing.ipynb Cell 47\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/nburley/Recurrent-Neuron-Transformer/exploration_and_testing.ipynb#Y142sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m step_size \u001b[39m=\u001b[39m \u001b[39m16\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/nburley/Recurrent-Neuron-Transformer/exploration_and_testing.ipynb#Y142sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/nburley/Recurrent-Neuron-Transformer/exploration_and_testing.ipynb#Y142sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m train_shakespeare_trainsformer(transformer_model, context_window, step_size, train_loader, \n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/nburley/Recurrent-Neuron-Transformer/exploration_and_testing.ipynb#Y142sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m                                optimizer\u001b[39m=\u001b[39;49mtransformer_optimizer, num_epochs\u001b[39m=\u001b[39;49mNUM_EPOCHS)\n",
      "\u001b[1;32m/home/nburley/Recurrent-Neuron-Transformer/exploration_and_testing.ipynb Cell 47\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/nburley/Recurrent-Neuron-Transformer/exploration_and_testing.ipynb#Y142sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m target_seq \u001b[39m=\u001b[39m target_seq\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/nburley/Recurrent-Neuron-Transformer/exploration_and_testing.ipynb#Y142sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m# Calculate loss and backpropagate\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/nburley/Recurrent-Neuron-Transformer/exploration_and_testing.ipynb#Y142sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m loss \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mCrossEntropyLoss()(outputs, target_seq)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/nburley/Recurrent-Neuron-Transformer/exploration_and_testing.ipynb#Y142sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/nburley/Recurrent-Neuron-Transformer/exploration_and_testing.ipynb#Y142sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> 1174\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m   1175\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[1;32m   1176\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/functional.py:3029\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3028\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3029\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 492.00 MiB (GPU 0; 10.00 GiB total capacity; 16.10 GiB already allocated; 0 bytes free; 16.13 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 completed. Loss: 108.20048586876838\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48f4bfa2fad24eb087a9f4bb5a954480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/5:   0%|          | 0/546 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 completed. Loss: 107.74317261238238\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49bc4640210448c7bd90e118836e15fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/5:   0%|          | 0/546 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 completed. Loss: 107.73876081892858\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65a78c5397f241f3abf82850d2e92052",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/5:   0%|          | 0/546 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 completed. Loss: 107.70513488696172\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d93e6d6c4e3d4dd885715cb343602219",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/5:   0%|          | 0/546 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 completed. Loss: 107.69152781378219\n"
      "torch.Size([100, 12, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1200x1 and 12x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\chris\\Desktop\\recurrent_neuron_transformers\\test_stateful_neuron.ipynb Cell 37\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/chris/Desktop/recurrent_neuron_transformers/test_stateful_neuron.ipynb#X51sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m seq_criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mMSELoss()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/chris/Desktop/recurrent_neuron_transformers/test_stateful_neuron.ipynb#X51sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m seq_optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(sequential_stateful_neuron_model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/chris/Desktop/recurrent_neuron_transformers/test_stateful_neuron.ipynb#X51sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m train_time_series(model\u001b[39m=\u001b[39;49msequential_stateful_neuron_model, device\u001b[39m=\u001b[39;49mDEVICE, train_loader\u001b[39m=\u001b[39;49msunspot_train_loader, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/chris/Desktop/recurrent_neuron_transformers/test_stateful_neuron.ipynb#X51sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m       val_loader\u001b[39m=\u001b[39;49msunspot_test_loader, criterion\u001b[39m=\u001b[39;49mseq_criterion, opt\u001b[39m=\u001b[39;49mseq_optimizer, epochs\u001b[39m=\u001b[39;49mNUM_EPOCHS)\n",
      "\u001b[1;32mc:\\Users\\chris\\Desktop\\recurrent_neuron_transformers\\test_stateful_neuron.ipynb Cell 37\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chris/Desktop/recurrent_neuron_transformers/test_stateful_neuron.ipynb#X51sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m opt\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chris/Desktop/recurrent_neuron_transformers/test_stateful_neuron.ipynb#X51sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# Forward pass and loss calculation\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/chris/Desktop/recurrent_neuron_transformers/test_stateful_neuron.ipynb#X51sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(sequences)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chris/Desktop/recurrent_neuron_transformers/test_stateful_neuron.ipynb#X51sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chris/Desktop/recurrent_neuron_transformers/test_stateful_neuron.ipynb#X51sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# Backward pass and weight update\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\chris\\Desktop\\recurrent_neuron_transformers\\test_stateful_neuron.ipynb Cell 37\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/chris/Desktop/recurrent_neuron_transformers/test_stateful_neuron.ipynb#X51sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(x\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chris/Desktop/recurrent_neuron_transformers/test_stateful_neuron.ipynb#X51sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, (neuron, linear_layer) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneurons[\u001b[39m1\u001b[39m:], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear_layers)):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/chris/Desktop/recurrent_neuron_transformers/test_stateful_neuron.ipynb#X51sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     x \u001b[39m=\u001b[39m linear_layer(x)  \u001b[39m# Using the nn.Linear layer here\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chris/Desktop/recurrent_neuron_transformers/test_stateful_neuron.ipynb#X51sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     x, _, new_hidden \u001b[39m=\u001b[39m neuron(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chris/Desktop/recurrent_neuron_transformers/test_stateful_neuron.ipynb#X51sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     neuron\u001b[39m.\u001b[39mhidden \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mParameter(new_hidden, requires_grad\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1200x1 and 12x128)"
     ]
    }
   ],
   "source": [
    "# Define the step size to use for the sliding window\n",
    "step_size = 16\n",
    "\n",
    "# Train the model\n",
    "train_shakespeare_trainsformer(transformer_model, context_window, step_size, train_loader, \n",
    "                               optimizer=transformer_optimizer, num_epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
