{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stateful Neuron Exploration\n",
    "\n",
    "This notebook will test the performance of the stateful neural network PyTorch port, to verify that the method is \n",
    "implemented correctly and behaves as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# General imports\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import copy\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Imports for the tokenizer, the dataset, and the model\n",
    "from transformers import GPT2Tokenizer\n",
    "from utils.datasets import TextDataLoader\n",
    "from models.transformer_model import TransformerModel\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# MNIST dataset hyperparameters\n",
    "MNIST_INPUT_SIZE = 784\n",
    "MNIST_LAYER_SIZES = [MNIST_INPUT_SIZE, 128, 64, 10]\n",
    "MNIST_BATCH_SIZE = 1000\n",
    "\n",
    "# Sunspot dataset hyperparameters\n",
    "SUNSPOT_INPUT_SIZE = 12\n",
    "SUNSPOT_LAYER_SIZES = [SUNSPOT_INPUT_SIZE, 128, 64, 1]\n",
    "SUNSPOT_BATCH_SIZE = 100\n",
    "\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "# Device configuration\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device('mps')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Set notebook to reload external python modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 1: Stateful Neuron vs. FCN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Dataset Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training set and loader\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "trainset = datasets.MNIST(root='./data/test/', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=MNIST_BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Create test set and loader\n",
    "testset = datasets.MNIST(root='./data/test/', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=MNIST_BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function: Train Network on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_data, test_data, criterion, opt, epochs):\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Training phase\n",
    "        model.train()  # Set the model to training mode\n",
    "        train_loss = 0.0\n",
    "        for inputs, labels in tqdm(train_data, desc=f'Training: Epoch {epoch+1}/{epochs}', unit='batch'):\n",
    "            # Move the data to the device\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Flatten the images\n",
    "            inputs = inputs.view(inputs.shape[0], -1)  # Flatten the images\n",
    "\n",
    "            # Zero the gradients\n",
    "            opt.zero_grad()\n",
    "\n",
    "            # Forward pass and loss calculation\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and weight update\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            # Logging the loss\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Testing phase\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        test_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_data:\n",
    "                # Move the data to the device\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Flatten the images\n",
    "                inputs = inputs.view(inputs.shape[0], -1)\n",
    "\n",
    "                # Forward pass and loss calculation\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Logging the loss and updating variables at batch level\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Logging the losses\n",
    "        train_loss /= len(trainloader)\n",
    "        test_loss /= len(testloader)\n",
    "        test_accuracy = 100 * correct / total\n",
    "        print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `StatefulNeuronNetwork`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neurons(nn.Module):\n",
    "    def __init__(self, n_neurons):\n",
    "        super(Neurons, self).__init__()\n",
    "\n",
    "        # Initialize matrix neuron parameters and number of neurons to create\n",
    "        self.n_neurons = n_neurons\n",
    "        self.params = nn.Parameter(torch.rand(n_neurons, 3, 3) * 2 - 1)\n",
    "\n",
    "        # Initialize hidden state for batch processing\n",
    "        self.hidden = nn.Parameter(torch.zeros(1, n_neurons, 1), requires_grad=False)\n",
    "    \n",
    "    def neuron_fn(self, inputs):\n",
    "        batch_size = inputs.shape[0]\n",
    "\n",
    "        # Expand hidden to match batch size\n",
    "        hidden_batch = self.hidden.expand(batch_size, -1, -1)\n",
    "\n",
    "        # Ensure inputs is 2D: (batch_size, n_neurons)\n",
    "        inputs = inputs.view(batch_size, -1, 1)\n",
    "        ones = torch.ones_like(inputs)\n",
    "\n",
    "        # Concatenate along the second dimension\n",
    "        stacked = torch.cat((inputs, hidden_batch, ones), dim=1)\n",
    "\n",
    "        # Reshape stacked for matrix multiplication: [batch_size, n_neurons, 3]\n",
    "        stacked = stacked.view(batch_size, self.n_neurons, 3)\n",
    "\n",
    "        # Perform matrix multiplication\n",
    "        dot = torch.tanh(torch.matmul(self.params, stacked.unsqueeze(3)).squeeze(3))\n",
    "\n",
    "        # Update hidden state\n",
    "        self.hidden = nn.Parameter(dot[:, :, -1].unsqueeze(2).detach(), requires_grad=False)\n",
    "\n",
    "        return dot[:, :, 0], dot\n",
    "\n",
    "class NeuralDiverseNet(nn.Module):\n",
    "    def __init__(self, sizes):\n",
    "        super(NeuralDiverseNet, self).__init__()\n",
    "        self.neurons = nn.ModuleList([Neurons(size) for size in sizes])\n",
    "        self.weights = nn.ModuleList([nn.Linear(sizes[i], sizes[i + 1]) for i in range(len(sizes) - 1)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        for i, neuron in enumerate(self.neurons[:-1]):  # Process through all but last layer\n",
    "            send, _ = neuron.neuron_fn(x if i == 0 else pre)\n",
    "            pre = self.weights[i](send)\n",
    "\n",
    "        # Process the last layer\n",
    "        final_output, _ = self.neurons[-1].neuron_fn(pre)\n",
    "\n",
    "        # Reshape the output to ensure it has the shape [batch_size, n_classes]\n",
    "        final_output = final_output.view(batch_size, -1)\n",
    "\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0346613618d648b2ad167d3b7e2585c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: Epoch 1/5:   0%|          | 0/60 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 2.1622, Test Loss: 1.9957, Test Accuracy: 37.46%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13d292e6201a499bba39d04a7b6c177c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: Epoch 2/5:   0%|          | 0/60 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Loss: 1.9257, Test Loss: 1.8472, Test Accuracy: 45.10%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f5cfab60c1d483abbbe113b41ddac14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: Epoch 3/5:   0%|          | 0/60 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train Loss: 1.8214, Test Loss: 1.7847, Test Accuracy: 46.48%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f2acedc0ee74a678c18318c4842051a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: Epoch 4/5:   0%|          | 0/60 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train Loss: 1.7844, Test Loss: 1.7631, Test Accuracy: 46.87%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6870efb9cd9e4c3fb40af8882ad73e3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: Epoch 5/5:   0%|          | 0/60 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Train Loss: 1.7631, Test Loss: 1.7447, Test Accuracy: 47.28%\n"
     ]
    }
   ],
   "source": [
    "stateful_neuron_model = NeuralDiverseNet(MNIST_LAYER_SIZES).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(stateful_neuron_model.parameters(), lr=0.001)\n",
    "\n",
    "train(model=stateful_neuron_model, device=DEVICE, train_data=trainloader, test_data=testloader, \n",
    "      criterion=criterion, opt=optimizer, epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `FeedforwardNetwork`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, MNIST_LAYER_SIZES):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(len(MNIST_LAYER_SIZES) - 1):\n",
    "            self.layers.append(nn.Linear(MNIST_LAYER_SIZES[i], MNIST_LAYER_SIZES[i + 1]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = torch.relu(layer(x))\n",
    "        x = self.layers[-1](x)  # No activation after the last layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6ef3dd67b7d403dbdb5db02005e50a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: Epoch 1/5:   0%|          | 0/60 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.9722, Test Loss: 0.4075, Test Accuracy: 88.50%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02a8f944f8d341c589125aa0e35df6bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: Epoch 2/5:   0%|          | 0/60 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Loss: 0.3653, Test Loss: 0.3129, Test Accuracy: 90.74%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad5788d86da64fb8ba82c7086e1ab08e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: Epoch 3/5:   0%|          | 0/60 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train Loss: 0.3053, Test Loss: 0.2760, Test Accuracy: 91.97%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "558edf3b4b3646e298c2504c2ba1217e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: Epoch 4/5:   0%|          | 0/60 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train Loss: 0.2691, Test Loss: 0.2475, Test Accuracy: 92.67%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35e9608dad28458ba2254007ede5cea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: Epoch 5/5:   0%|          | 0/60 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Train Loss: 0.2407, Test Loss: 0.2190, Test Accuracy: 93.44%\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "ff_model = FeedForwardNetwork(MNIST_LAYER_SIZES).to(DEVICE)\n",
    "ff_optimizer = torch.optim.Adam(ff_model.parameters(), lr=0.001)\n",
    "ff_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "train(model=ff_model, device=DEVICE, train_data=trainloader, test_data=testloader, \n",
    "      criterion=ff_criterion, opt=ff_optimizer, epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `SpikingNeuralNetwork`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def surrogate_gradient(x):\n",
    "    alpha = 10  # The steepness of the surrogate gradient\n",
    "    return torch.sigmoid(alpha * x)\n",
    "\n",
    "class SpikingNeuronLayer(nn.Module):\n",
    "    def __init__(self, size_in, size_out, device):\n",
    "        super(SpikingNeuronLayer, self).__init__()\n",
    "        self.device = device\n",
    "        self.synaptic_weights = nn.Parameter(torch.randn(size_in, size_out, device=device) * 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(self.device)\n",
    "        pre_synaptic = torch.matmul(x, self.synaptic_weights)\n",
    "        post_synaptic = surrogate_gradient(pre_synaptic - 1)\n",
    "        return post_synaptic\n",
    "\n",
    "class SpikingNeuralNetwork(nn.Module):\n",
    "    def __init__(self, MNIST_LAYER_SIZES, device):\n",
    "        super(SpikingNeuralNetwork, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.device = device\n",
    "        for i in range(len(MNIST_LAYER_SIZES) - 1):\n",
    "            self.layers.append(SpikingNeuronLayer(MNIST_LAYER_SIZES[i], MNIST_LAYER_SIZES[i + 1], device))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(self.device)  # Ensure input tensor is on the correct device\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fd857676a844b8b941cbf5de8859408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: Epoch 1/5:   0%|          | 0/60 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 2.3026, Test Loss: 2.3026, Test Accuracy: 11.35%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0b2e0051ee9487987520e15be69c63d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: Epoch 2/5:   0%|          | 0/60 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Loss: 2.3026, Test Loss: 2.3026, Test Accuracy: 11.35%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "644b3ef360ba487f96fd5c5b53a66656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: Epoch 3/5:   0%|          | 0/60 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train Loss: 2.3026, Test Loss: 2.3026, Test Accuracy: 11.35%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c8f61f33f38425285e36a0bfa8c95db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: Epoch 4/5:   0%|          | 0/60 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train Loss: 2.3026, Test Loss: 2.3026, Test Accuracy: 11.35%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "258acc5e83f84c5e9a04069e8bcc9c86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: Epoch 5/5:   0%|          | 0/60 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Train Loss: 2.3026, Test Loss: 2.3026, Test Accuracy: 11.35%\n"
     ]
    }
   ],
   "source": [
    "# Create the network\n",
    "snn_model = SpikingNeuralNetwork([784, 128, 64, 10], device=DEVICE).to(DEVICE)\n",
    "snn_optimizer = torch.optim.Adam(snn_model.parameters(), lr=0.001)\n",
    "snn_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "train(model=snn_model, device=DEVICE, train_data=trainloader, test_data=testloader, \n",
    "      criterion=snn_criterion, opt=snn_optimizer, epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 2: Stateful Neurons vs. RNN\n",
    "Do stateful neurons perform similarly to recurrent neural networks on a simple time series task?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length: 2400\n",
      "Testing set length: 800\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('data/test/Sunspots.csv', usecols=['Monthly Mean Total Sunspot Number'])\n",
    "data = df.values.astype(float)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data_normalized = scaler.fit_transform(data)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "data_normalized = torch.FloatTensor(data_normalized).view(-1)\n",
    "\n",
    "# Create sequences and corresponding labels\n",
    "sequence_length = 12  # For example, use 12 months to predict the next month\n",
    "sequences = []\n",
    "labels = []\n",
    "\n",
    "for i in range(len(data_normalized) - sequence_length):\n",
    "    sequences.append(data_normalized[i:i+sequence_length])\n",
    "    labels.append(data_normalized[i+sequence_length])\n",
    "\n",
    "sequences = torch.stack(sequences[:-1])\n",
    "labels = torch.stack(labels[1:])\n",
    "\n",
    "# Split the data into training, validation, and testing sets\n",
    "train_sequences, test_sequences, train_labels, test_labels = train_test_split(\n",
    "    sequences, labels, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "# Trim the dataset to an easily divisible length\n",
    "train_sequences = train_sequences[:2400]\n",
    "train_labels = train_labels[:2400]\n",
    "test_sequences = test_sequences[:800]\n",
    "test_labels = test_labels[:800]\n",
    "\n",
    "# Create DataLoaders for each set\n",
    "sunspot_train_loader = DataLoader(TensorDataset(train_sequences, train_labels), shuffle=True, \n",
    "                                  batch_size=SUNSPOT_BATCH_SIZE)\n",
    "sunspot_test_loader = DataLoader(TensorDataset(test_sequences, test_labels), shuffle=False, \n",
    "                                 batch_size=SUNSPOT_BATCH_SIZE)\n",
    "\n",
    "# Print the length of each set\n",
    "print(f'Training set length: {len(train_sequences)}')\n",
    "print(f'Testing set length: {len(test_sequences)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function: Train Network on Sunspot Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_time_series(model, train_loader, val_loader, criterion, opt, epochs, device=DEVICE, model_type=None):\n",
    "    # Perform training\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for sequences, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}', unit='batch'):\n",
    "\n",
    "            # Move the data to the device and reshape the targets\n",
    "            sequences, labels = sequences.to(device), labels.to(device)\n",
    "            sequences = sequences.view(sequences.shape[0], SUNSPOT_INPUT_SIZE, 1).to(device)\n",
    "            labels = labels.unsqueeze(1).to(device)  # Reshape targets\n",
    "\n",
    "            # Zero the gradients\n",
    "            opt.zero_grad()\n",
    "\n",
    "            # Forward pass and loss calculation\n",
    "            outputs = model(sequences)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and weight update\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            # Logging the loss\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for sequences, labels in val_loader:\n",
    "                # Move the data to the device and reshape the targets\n",
    "                sequences, labels = sequences.to(device), labels.to(device)\n",
    "                sequences = sequences.view(sequences.shape[0], SUNSPOT_INPUT_SIZE, 1).to(device)\n",
    "                labels = labels.unsqueeze(1).to(device)  # Reshape targets\n",
    "\n",
    "                # Forward pass and loss calculation\n",
    "                outputs = model(sequences)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f\"Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `RecurrentNeuralNetwork`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaRNN(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=12, output_size=1):\n",
    "        super(VanillaRNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.fc(out[:, -1, :])  # Using the last time step's output\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42ad4611e6f04a32989456c4b4d7616d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/5:   0%|          | 0/24 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6743, Validation Loss: 0.6094\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c37ba17b9054c95b53d548f697a6c53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/5:   0%|          | 0/24 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5567, Validation Loss: 0.5024\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d22d293f9d24a55a4abd3b9ba7eec53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/5:   0%|          | 0/24 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.4586, Validation Loss: 0.4129\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84c4a847e2e54524b1ea51c9f652a635",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/5:   0%|          | 0/24 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.3774, Validation Loss: 0.3406\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f7a68e47e6441bdb1c0db5d76adb83e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/5:   0%|          | 0/24 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.3114, Validation Loss: 0.2816\n"
     ]
    }
   ],
   "source": [
    "rnn_model = VanillaRNN(input_size=1, hidden_size=1, output_size=1).to(DEVICE)\n",
    "rnn_criterion = nn.MSELoss()\n",
    "rnn_optimizer = torch.optim.Adam(rnn_model.parameters(), lr=0.001)\n",
    "\n",
    "train_time_series(model=rnn_model, model_type='RNN', device=DEVICE, train_loader=sunspot_train_loader, \n",
    "      val_loader=sunspot_test_loader, criterion=rnn_criterion, opt=rnn_optimizer, epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `RecurrentStatefulNeuron`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neurons(nn.Module):\n",
    "    def __init__(self, n_neurons):\n",
    "        super(Neurons, self).__init__()\n",
    "\n",
    "        # Initialize matrix neuron parameters and number of neurons to create\n",
    "        self.n_neurons = n_neurons\n",
    "        self.params = nn.Parameter(torch.rand(n_neurons, 3, 3) * 2 - 1)\n",
    "\n",
    "        # Initialize hidden state for batch processing\n",
    "        self.hidden = nn.Parameter(torch.zeros(1, n_neurons, 1), requires_grad=False)\n",
    "    \n",
    "    def neuron_fn(self, inputs):\n",
    "        batch_size = inputs.shape[0]\n",
    "\n",
    "        # Expand hidden to match batch size\n",
    "        hidden_batch = self.hidden.expand(batch_size, -1, -1)\n",
    "\n",
    "        # Ensure inputs is 2D: (batch_size, n_neurons)\n",
    "        inputs = inputs.view(batch_size, -1, 1)\n",
    "        ones = torch.ones_like(inputs)\n",
    "\n",
    "        # Concatenate along the second dimension\n",
    "        stacked = torch.cat((inputs, hidden_batch, ones), dim=1)\n",
    "\n",
    "        # Reshape stacked for matrix multiplication: [batch_size, n_neurons, 3]\n",
    "        stacked = stacked.view(batch_size, self.n_neurons, 3)\n",
    "\n",
    "        # Perform matrix multiplication\n",
    "        dot = torch.tanh(torch.matmul(self.params, stacked.unsqueeze(3)).squeeze(3))\n",
    "\n",
    "        # Update hidden state\n",
    "        self.hidden = nn.Parameter(dot[:, :, -1].unsqueeze(2).detach(), requires_grad=False)\n",
    "\n",
    "        return dot[:, :, 0], dot\n",
    "\n",
    "class SequentialNeuralDiverseNet(nn.Module):\n",
    "    def __init__(self, sizes):\n",
    "        super(SequentialNeuralDiverseNet, self).__init__()\n",
    "        self.neurons = nn.ModuleList([Neurons(size) for size in sizes])\n",
    "        self.weights = nn.ModuleList([nn.Linear(sizes[i], sizes[i + 1]) for i in range(len(sizes) - 1)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        for i, neuron in enumerate(self.neurons[:-1]):\n",
    "            send, _ = neuron.neuron_fn(x if i == 0 else pre)\n",
    "            pre = self.weights[i](send)\n",
    "\n",
    "        final_output, _ = self.neurons[-1].neuron_fn(pre)\n",
    "\n",
    "        # Since we're predicting a single value, we reshape the output to [batch_size, 1]\n",
    "        final_output = final_output.view(batch_size, -1)\n",
    "\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03202352d3844fb4849269bad072c945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/5:   0%|          | 0/24 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.3925, Validation Loss: 0.2892\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f98da935358047bd9b6b3f2f2d354c48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/5:   0%|          | 0/24 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1542, Validation Loss: 0.0373\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7535a9797d842f08bca8c50ce883627",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/5:   0%|          | 0/24 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0322, Validation Loss: 0.0310\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a5588c28e694c0aad1808e902fc8714",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/5:   0%|          | 0/24 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0299, Validation Loss: 0.0291\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f5a180a28744ab3ac87bf2117309859",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/5:   0%|          | 0/24 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0294, Validation Loss: 0.0290\n"
     ]
    }
   ],
   "source": [
    "sequential_stateful_neuron_model = SequentialNeuralDiverseNet(SUNSPOT_LAYER_SIZES).to(DEVICE)\n",
    "seq_criterion = nn.MSELoss()\n",
    "seq_optimizer = torch.optim.Adam(sequential_stateful_neuron_model.parameters(), lr=0.001)\n",
    "\n",
    "train_time_series(model=sequential_stateful_neuron_model, device=DEVICE, train_loader=sunspot_train_loader, \n",
    "      val_loader=sunspot_test_loader, criterion=seq_criterion, opt=seq_optimizer, epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 3: Train Transformer On Shakespeare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define helper function to train the transformer on reconstructing the Shakespeare dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to mask the target tokens\n",
    "# TODO: Using a mask breaks the training process due to a shape error. Needs to be fixed\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = torch.triu(torch.ones(size, size), diagonal=1)\n",
    "    return mask.masked_fill(mask == 1, float('-inf')).masked_fill(mask == 0, float(0.0))\n",
    "\n",
    "def train_shakespeare_trainsformer(model, context_window, step_size, data_loader, optimizer, num_epochs, device=DEVICE, mask=False):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for inputs, labels in tqdm(data_loader, desc=f'Training: Epoch {epoch+1}/{num_epochs}', unit='batch'):\n",
    "            # Move the data to the device\n",
    "            input_seq = inputs.to(device)\n",
    "            target_seq = labels.to(device)\n",
    "            \n",
    "            # Optionally create a mask for the target sequence\n",
    "            if mask == True:\n",
    "                target_seq_mask = create_look_ahead_mask(target_seq.size(1)).to(device)\n",
    "                target_seq_mask = target_seq_mask.unsqueeze(0)\n",
    "            else:\n",
    "                target_seq_mask = None\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_seq, target_seq, tgt_mask=target_seq_mask)\n",
    "            outputs = outputs.view(-1, outputs.size(-1))\n",
    "            target_seq = target_seq.view(-1)\n",
    "\n",
    "            # Calculate loss and backpropagate\n",
    "            loss = nn.CrossEntropyLoss()(outputs, target_seq)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Logging the loss and update progress bar\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} completed. Loss: {epoch_loss/len(data_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Shakespeare dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "__len__() should return >= 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/nburley/Recurrent-Neuron-Transformer/exploration_and_testing.ipynb Cell 43\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/nburley/Recurrent-Neuron-Transformer/exploration_and_testing.ipynb#Y135sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Create the data loader\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/nburley/Recurrent-Neuron-Transformer/exploration_and_testing.ipynb#Y135sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m data_loader \u001b[39m=\u001b[39m TextDataLoader(file_path, seq_length, bpe_tokenizer, batch_size, vocab_size)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/nburley/Recurrent-Neuron-Transformer/exploration_and_testing.ipynb#Y135sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m train_loader, test_loader \u001b[39m=\u001b[39m data_loader\u001b[39m.\u001b[39;49mcreate_loaders()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/nburley/Recurrent-Neuron-Transformer/exploration_and_testing.ipynb#Y135sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m data_loader \u001b[39m=\u001b[39m TextDataLoader(file_path, seq_length, bpe_tokenizer, batch_size, vocab_size)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/nburley/Recurrent-Neuron-Transformer/exploration_and_testing.ipynb#Y135sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m train_loader, test_loader \u001b[39m=\u001b[39m data_loader\u001b[39m.\u001b[39mcreate_loaders()\n",
      "File \u001b[0;32m~/Recurrent-Neuron-Transformer/utils/datasets.py:55\u001b[0m, in \u001b[0;36mTextDataLoader.create_loaders\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_loaders\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     54\u001b[0m     train_dataset, test_dataset \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_datasets()\n\u001b[0;32m---> 55\u001b[0m     train_loader \u001b[39m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_size, shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, drop_last\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     56\u001b[0m     test_loader \u001b[39m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size, shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, drop_last\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     57\u001b[0m     \u001b[39mreturn\u001b[39;00m train_loader, test_loader\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:351\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# map-style\u001b[39;00m\n\u001b[1;32m    350\u001b[0m     \u001b[39mif\u001b[39;00m shuffle:\n\u001b[0;32m--> 351\u001b[0m         sampler \u001b[39m=\u001b[39m RandomSampler(dataset, generator\u001b[39m=\u001b[39;49mgenerator)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    352\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    353\u001b[0m         sampler \u001b[39m=\u001b[39m SequentialSampler(dataset)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/utils/data/sampler.py:106\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplacement, \u001b[39mbool\u001b[39m):\n\u001b[1;32m    103\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mreplacement should be a boolean value, but got \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    104\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39mreplacement=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplacement))\n\u001b[0;32m--> 106\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_samples, \u001b[39mint\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_samples \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    107\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mnum_samples should be a positive integer \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    108\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39mvalue, but got num_samples=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_samples))\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/utils/data/sampler.py:114\u001b[0m, in \u001b[0;36mRandomSampler.num_samples\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    111\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnum_samples\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[1;32m    112\u001b[0m     \u001b[39m# dataset size might change at runtime\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_samples \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata_source)\n\u001b[1;32m    115\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_samples\n",
      "\u001b[0;31mValueError\u001b[0m: __len__() should return >= 0"
     ]
    }
   ],
   "source": [
    "# # Define tokenizer used to convert text to tokens\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Define dataset parameters\n",
    "seq_length = 1024\n",
    "batch_size = 5\n",
    "file_path = os.path.join(os.getcwd(), 'data/shakespeare', 'tinyshakespeare.txt')\n",
    "bpe_tokenizer = 'gpt2'\n",
    "vocab_size = 50257\n",
    "\n",
    "# Create the data loader\n",
    "data_loader = TextDataLoader(file_path, seq_length, bpe_tokenizer, batch_size, vocab_size)\n",
    "train_loader, test_loader = data_loader.create_loaders()\n",
    "data_loader = TextDataLoader(file_path, seq_length, bpe_tokenizer, batch_size, vocab_size)\n",
    "train_loader, test_loader = data_loader.create_loaders()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the transformer model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the context window size k (defaulting to chunk_length / 2)\n",
    "context_window = 512\n",
    "\n",
    "# Define the model\n",
    "transformer_model = TransformerModel(vocab_size=tokenizer.vocab_size, max_seq_length=context_window).to(DEVICE)\n",
    "\n",
    "# Define optimizer\n",
    "transformer_optimizer = torch.optim.Adam(transformer_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b880af3cb68e4be896467389b94d5374",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: Epoch 1/5:   0%|          | 0/14 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 completed. Loss: 4.751185859952654\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dead47eeefd4f2c916c840061e56afe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: Epoch 2/5:   0%|          | 0/14 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 completed. Loss: 4.750765357698713\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "860e668ed053413ba29d34cb74b754e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: Epoch 3/5:   0%|          | 0/14 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 completed. Loss: 4.751244306564331\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf757eb9bbc14db0996004c1e2c967e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: Epoch 4/5:   0%|          | 0/14 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 completed. Loss: 4.754691294261387\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73b6102ec8f04630a7feacda57fabab6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: Epoch 5/5:   0%|          | 0/14 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 completed. Loss: 4.751231908798218\n"
     ]
    }
   ],
   "source": [
    "# Define the step size to use for the sliding window\n",
    "step_size = 64\n",
    "\n",
    "# Train the model\n",
    "train_shakespeare_trainsformer(transformer_model, context_window, step_size, train_loader, \n",
    "                               optimizer=transformer_optimizer, num_epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
