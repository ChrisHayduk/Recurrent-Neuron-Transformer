{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stateful Neuron Exploration\n",
    "\n",
    "This notebook will test the performance of the stateful neural network PyTorch port, to verify that the method is \n",
    "implemented correctly and behaves as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import copy\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Imports for the tokenizer, the dataset, and the model\n",
    "from transformers import GPT2Tokenizer\n",
    "from utils.datasets import ShakespeareDataset\n",
    "from models.transformer_model import TransformerModel\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# MNIST dataset hyperparameters\n",
    "MNIST_INPUT_SIZE = 784\n",
    "MNIST_LAYER_SIZES = [MNIST_INPUT_SIZE, 128, 64, 10]\n",
    "MNIST_BATCH_SIZE = 1000\n",
    "\n",
    "# Sunspot dataset hyperparameters\n",
    "SUNSPOT_INPUT_SIZE = 12\n",
    "SUNSPOT_LAYER_SIZES = [SUNSPOT_INPUT_SIZE, 128, 64, 1]\n",
    "SUNSPOT_BATCH_SIZE = 100\n",
    "\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "# Device configuration\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device('mps')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Set notebook to reload external python modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 1: Stateful Neuron vs. FCN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Dataset Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training set and loader\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "trainset = datasets.MNIST(root='./data/test/', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=MNIST_BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Create test set and loader\n",
    "testset = datasets.MNIST(root='./data/test/', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=MNIST_BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function: Train Network on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_data, test_data, criterion, opt, epochs):\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Training phase\n",
    "        model.train()  # Set the model to training mode\n",
    "        train_loss = 0.0\n",
    "        for inputs, labels in tqdm(train_data, desc=f'Training: Epoch {epoch+1}/{epochs}', unit='batch'):\n",
    "            # Move the data to the device\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Flatten the images\n",
    "            inputs = inputs.view(inputs.shape[0], -1)  # Flatten the images\n",
    "\n",
    "            # Zero the gradients\n",
    "            opt.zero_grad()\n",
    "\n",
    "            # Forward pass and loss calculation\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and weight update\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            # Logging the loss\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Testing phase\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        test_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_data:\n",
    "                # Move the data to the device\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Flatten the images\n",
    "                inputs = inputs.view(inputs.shape[0], -1)\n",
    "\n",
    "                # Forward pass and loss calculation\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Logging the loss and updating variables at batch level\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Logging the losses\n",
    "        train_loss /= len(trainloader)\n",
    "        test_loss /= len(testloader)\n",
    "        test_accuracy = 100 * correct / total\n",
    "        print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `StatefulNeuronNetwork`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neurons(nn.Module):\n",
    "    def __init__(self, n_neurons):\n",
    "        super(Neurons, self).__init__()\n",
    "\n",
    "        # Initialize matrix neuron parameters and number of neurons to create\n",
    "        self.n_neurons = n_neurons\n",
    "        self.params = nn.Parameter(torch.rand(n_neurons, 3, 3) * 2 - 1)\n",
    "\n",
    "        # Initialize hidden state for batch processing\n",
    "        self.hidden = nn.Parameter(torch.zeros(1, n_neurons, 1), requires_grad=False)\n",
    "    \n",
    "    def neuron_fn(self, inputs):\n",
    "        batch_size = inputs.shape[0]\n",
    "\n",
    "        # Expand hidden to match batch size\n",
    "        hidden_batch = self.hidden.expand(batch_size, -1, -1)\n",
    "\n",
    "        # Ensure inputs is 2D: (batch_size, n_neurons)\n",
    "        inputs = inputs.view(batch_size, -1, 1)\n",
    "        ones = torch.ones_like(inputs)\n",
    "\n",
    "        # Concatenate along the second dimension\n",
    "        stacked = torch.cat((inputs, hidden_batch, ones), dim=1)\n",
    "\n",
    "        # Reshape stacked for matrix multiplication: [batch_size, n_neurons, 3]\n",
    "        stacked = stacked.view(batch_size, self.n_neurons, 3)\n",
    "\n",
    "        # Perform matrix multiplication\n",
    "        dot = torch.tanh(torch.matmul(self.params, stacked.unsqueeze(3)).squeeze(3))\n",
    "\n",
    "        # Update hidden state\n",
    "        self.hidden = nn.Parameter(dot[:, :, -1].unsqueeze(2).detach(), requires_grad=False)\n",
    "\n",
    "        return dot[:, :, 0], dot\n",
    "\n",
    "class NeuralDiverseNet(nn.Module):\n",
    "    def __init__(self, sizes):\n",
    "        super(NeuralDiverseNet, self).__init__()\n",
    "        self.neurons = nn.ModuleList([Neurons(size) for size in sizes])\n",
    "        self.weights = nn.ModuleList([nn.Linear(sizes[i], sizes[i + 1]) for i in range(len(sizes) - 1)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        for i, neuron in enumerate(self.neurons[:-1]):  # Process through all but last layer\n",
    "            send, _ = neuron.neuron_fn(x if i == 0 else pre)\n",
    "            pre = self.weights[i](send)\n",
    "\n",
    "        # Process the last layer\n",
    "        final_output, _ = self.neurons[-1].neuron_fn(pre)\n",
    "\n",
    "        # Reshape the output to ensure it has the shape [batch_size, n_classes]\n",
    "        final_output = final_output.view(batch_size, -1)\n",
    "\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3111e7203838445eae2a036a8ad470e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: Epoch 1/5:   0%|          | 0/60 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 2.1622, Test Loss: 1.9957, Test Accuracy: 37.46%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84ca89816b374d838a544b238ae24b22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: Epoch 2/5:   0%|          | 0/60 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Loss: 1.9257, Test Loss: 1.8472, Test Accuracy: 45.10%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f80b89e924c4cf39d7caa6ee755c64b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: Epoch 3/5:   0%|          | 0/60 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train Loss: 1.8214, Test Loss: 1.7847, Test Accuracy: 46.48%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b09e5ec13d344b699e2ad052a249f21c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: Epoch 4/5:   0%|          | 0/60 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train Loss: 1.7844, Test Loss: 1.7631, Test Accuracy: 46.87%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8372b63649948cf8adea8981c9bb09a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: Epoch 5/5:   0%|          | 0/60 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Train Loss: 1.7631, Test Loss: 1.7447, Test Accuracy: 47.28%\n"
     ]
    }
   ],
   "source": [
    "stateful_neuron_model = NeuralDiverseNet(MNIST_LAYER_SIZES).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(stateful_neuron_model.parameters(), lr=0.001)\n",
    "\n",
    "train(model=stateful_neuron_model, device=DEVICE, train_data=trainloader, test_data=testloader, \n",
    "      criterion=criterion, opt=optimizer, epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `FeedforwardNetwork`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, MNIST_LAYER_SIZES):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(len(MNIST_LAYER_SIZES) - 1):\n",
    "            self.layers.append(nn.Linear(MNIST_LAYER_SIZES[i], MNIST_LAYER_SIZES[i + 1]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = torch.relu(layer(x))\n",
    "        x = self.layers[-1](x)  # No activation after the last layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1be728f7076a4ce782dd9fefed54786f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: Epoch 1/5:   0%|          | 0/60 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.9722, Test Loss: 0.4075, Test Accuracy: 88.50%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d493a2e26ce46f387540a08f3633790",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: Epoch 2/5:   0%|          | 0/60 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Loss: 0.3653, Test Loss: 0.3129, Test Accuracy: 90.74%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1fa78f100cb45a7887f73c112b5b50d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: Epoch 3/5:   0%|          | 0/60 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train Loss: 0.3053, Test Loss: 0.2760, Test Accuracy: 91.97%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51c62e4c32384a1db2bf2e2549ca04ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: Epoch 4/5:   0%|          | 0/60 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train Loss: 0.2691, Test Loss: 0.2475, Test Accuracy: 92.67%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "062dc81dab934def9c17fb7be50a385f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: Epoch 5/5:   0%|          | 0/60 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Train Loss: 0.2407, Test Loss: 0.2190, Test Accuracy: 93.44%\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "ff_model = FeedForwardNetwork(MNIST_LAYER_SIZES).to(DEVICE)\n",
    "ff_optimizer = torch.optim.Adam(ff_model.parameters(), lr=0.001)\n",
    "ff_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "train(model=ff_model, device=DEVICE, train_data=trainloader, test_data=testloader, \n",
    "      criterion=ff_criterion, opt=ff_optimizer, epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `SpikingNeuralNetwork`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def surrogate_gradient(x):\n",
    "    alpha = 10  # The steepness of the surrogate gradient\n",
    "    return torch.sigmoid(alpha * x)\n",
    "\n",
    "class SpikingNeuronLayer(nn.Module):\n",
    "    def __init__(self, size_in, size_out, device):\n",
    "        super(SpikingNeuronLayer, self).__init__()\n",
    "        self.device = device\n",
    "        self.synaptic_weights = nn.Parameter(torch.randn(size_in, size_out, device=device) * 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(self.device)\n",
    "        pre_synaptic = torch.matmul(x, self.synaptic_weights)\n",
    "        post_synaptic = surrogate_gradient(pre_synaptic - 1)\n",
    "        return post_synaptic\n",
    "\n",
    "class SpikingNeuralNetwork(nn.Module):\n",
    "    def __init__(self, MNIST_LAYER_SIZES, device):\n",
    "        super(SpikingNeuralNetwork, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.device = device\n",
    "        for i in range(len(MNIST_LAYER_SIZES) - 1):\n",
    "            self.layers.append(SpikingNeuronLayer(MNIST_LAYER_SIZES[i], MNIST_LAYER_SIZES[i + 1], device))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(self.device)  # Ensure input tensor is on the correct device\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ce19ed7ef144c399750a32df64ecd63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: Epoch 1/5:   0%|          | 0/60 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 2.3026, Test Loss: 2.3026, Test Accuracy: 11.35%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baf4d0b7c807456ea15358ed4d75e5ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: Epoch 2/5:   0%|          | 0/60 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Loss: 2.3026, Test Loss: 2.3026, Test Accuracy: 11.35%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d664001ac1043749a984ff3beb568ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: Epoch 3/5:   0%|          | 0/60 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train Loss: 2.3026, Test Loss: 2.3026, Test Accuracy: 11.35%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8e0fa7c758141dba3d683d60c53ed20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: Epoch 4/5:   0%|          | 0/60 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train Loss: 2.3026, Test Loss: 2.3026, Test Accuracy: 11.35%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "486b8060f1b34ce78e2124a12c04a2c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: Epoch 5/5:   0%|          | 0/60 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Train Loss: 2.3026, Test Loss: 2.3026, Test Accuracy: 11.35%\n"
     ]
    }
   ],
   "source": [
    "# Create the network\n",
    "snn_model = SpikingNeuralNetwork([784, 128, 64, 10], device=DEVICE).to(DEVICE)\n",
    "snn_optimizer = torch.optim.Adam(snn_model.parameters(), lr=0.001)\n",
    "snn_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "train(model=snn_model, device=DEVICE, train_data=trainloader, test_data=testloader, \n",
    "      criterion=snn_criterion, opt=snn_optimizer, epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 2: Stateful Neurons vs. RNN\n",
    "Do stateful neurons perform similarly to recurrent neural networks on a simple time series task?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length: 2400\n",
      "Testing set length: 800\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('data/test/Sunspots.csv', usecols=['Monthly Mean Total Sunspot Number'])\n",
    "data = df.values.astype(float)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data_normalized = scaler.fit_transform(data)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "data_normalized = torch.FloatTensor(data_normalized).view(-1)\n",
    "\n",
    "# Create sequences and corresponding labels\n",
    "sequence_length = 12  # For example, use 12 months to predict the next month\n",
    "sequences = []\n",
    "labels = []\n",
    "\n",
    "for i in range(len(data_normalized) - sequence_length):\n",
    "    sequences.append(data_normalized[i:i+sequence_length])\n",
    "    labels.append(data_normalized[i+sequence_length])\n",
    "\n",
    "sequences = torch.stack(sequences[:-1])\n",
    "labels = torch.stack(labels[1:])\n",
    "\n",
    "# Split the data into training, validation, and testing sets\n",
    "train_sequences, test_sequences, train_labels, test_labels = train_test_split(\n",
    "    sequences, labels, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "# Trim the dataset to an easily divisible length\n",
    "train_sequences = train_sequences[:2400]\n",
    "train_labels = train_labels[:2400]\n",
    "test_sequences = test_sequences[:800]\n",
    "test_labels = test_labels[:800]\n",
    "\n",
    "# Create DataLoaders for each set\n",
    "sunspot_train_loader = DataLoader(TensorDataset(train_sequences, train_labels), shuffle=True, \n",
    "                                  batch_size=SUNSPOT_BATCH_SIZE)\n",
    "sunspot_test_loader = DataLoader(TensorDataset(test_sequences, test_labels), shuffle=False, \n",
    "                                 batch_size=SUNSPOT_BATCH_SIZE)\n",
    "\n",
    "# Print the length of each set\n",
    "print(f'Training set length: {len(train_sequences)}')\n",
    "print(f'Testing set length: {len(test_sequences)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function: Train Network on Sunspot Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_time_series(model, train_loader, val_loader, criterion, opt, epochs, device=DEVICE, model_type=None):\n",
    "    # Perform training\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for sequences, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}', unit='batch'):\n",
    "\n",
    "            # Move the data to the device and reshape the targets\n",
    "            sequences, labels = sequences.to(device), labels.to(device)\n",
    "            sequences = sequences.view(sequences.shape[0], SUNSPOT_INPUT_SIZE, 1).to(device)\n",
    "            labels = labels.unsqueeze(1).to(device)  # Reshape targets\n",
    "\n",
    "            # Zero the gradients\n",
    "            opt.zero_grad()\n",
    "\n",
    "            # Forward pass and loss calculation\n",
    "            outputs = model(sequences)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and weight update\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            # Logging the loss\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for sequences, labels in val_loader:\n",
    "                # Move the data to the device and reshape the targets\n",
    "                sequences, labels = sequences.to(device), labels.to(device)\n",
    "                sequences = sequences.view(sequences.shape[0], SUNSPOT_INPUT_SIZE, 1).to(device)\n",
    "                labels = labels.unsqueeze(1).to(device)  # Reshape targets\n",
    "\n",
    "                # Forward pass and loss calculation\n",
    "                outputs = model(sequences)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f\"Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `RecurrentNeuralNetwork`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaRNN(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=12, output_size=1):\n",
    "        super(VanillaRNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.fc(out[:, -1, :])  # Using the last time step's output\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8efe8c9430d4e708e8c10b0b79ae5e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/5:   0%|          | 0/24 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6743, Validation Loss: 0.6094\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "044c4752b2be4114a0230ebe4f611afb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/5:   0%|          | 0/24 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5567, Validation Loss: 0.5024\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9337fdd2b0f487d946bcd57666e8327",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/5:   0%|          | 0/24 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.4586, Validation Loss: 0.4129\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57863cd8032d4f10b962abbf616f91e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/5:   0%|          | 0/24 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.3774, Validation Loss: 0.3406\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "993598a368154c01ad7692fad64fdc13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/5:   0%|          | 0/24 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.3114, Validation Loss: 0.2816\n"
     ]
    }
   ],
   "source": [
    "rnn_model = VanillaRNN(input_size=1, hidden_size=1, output_size=1).to(DEVICE)\n",
    "rnn_criterion = nn.MSELoss()\n",
    "rnn_optimizer = torch.optim.Adam(rnn_model.parameters(), lr=0.001)\n",
    "\n",
    "train_time_series(model=rnn_model, model_type='RNN', device=DEVICE, train_loader=sunspot_train_loader, \n",
    "      val_loader=sunspot_test_loader, criterion=rnn_criterion, opt=rnn_optimizer, epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `RecurrentStatefulNeuron`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neurons(nn.Module):\n",
    "    def __init__(self, n_neurons):\n",
    "        super(Neurons, self).__init__()\n",
    "\n",
    "        # Initialize matrix neuron parameters and number of neurons to create\n",
    "        self.n_neurons = n_neurons\n",
    "        self.params = nn.Parameter(torch.rand(n_neurons, 3, 3) * 2 - 1)\n",
    "\n",
    "        # Initialize hidden state for batch processing\n",
    "        self.hidden = nn.Parameter(torch.zeros(1, n_neurons, 1), requires_grad=False)\n",
    "    \n",
    "    def neuron_fn(self, inputs):\n",
    "        batch_size = inputs.shape[0]\n",
    "\n",
    "        # Expand hidden to match batch size\n",
    "        hidden_batch = self.hidden.expand(batch_size, -1, -1)\n",
    "\n",
    "        # Ensure inputs is 2D: (batch_size, n_neurons)\n",
    "        inputs = inputs.view(batch_size, -1, 1)\n",
    "        ones = torch.ones_like(inputs)\n",
    "\n",
    "        # Concatenate along the second dimension\n",
    "        stacked = torch.cat((inputs, hidden_batch, ones), dim=1)\n",
    "\n",
    "        # Reshape stacked for matrix multiplication: [batch_size, n_neurons, 3]\n",
    "        stacked = stacked.view(batch_size, self.n_neurons, 3)\n",
    "\n",
    "        # Perform matrix multiplication\n",
    "        dot = torch.tanh(torch.matmul(self.params, stacked.unsqueeze(3)).squeeze(3))\n",
    "\n",
    "        # Update hidden state\n",
    "        self.hidden = nn.Parameter(dot[:, :, -1].unsqueeze(2).detach(), requires_grad=False)\n",
    "\n",
    "        return dot[:, :, 0], dot\n",
    "\n",
    "class SequentialNeuralDiverseNet(nn.Module):\n",
    "    def __init__(self, sizes):\n",
    "        super(SequentialNeuralDiverseNet, self).__init__()\n",
    "        self.neurons = nn.ModuleList([Neurons(size) for size in sizes])\n",
    "        self.weights = nn.ModuleList([nn.Linear(sizes[i], sizes[i + 1]) for i in range(len(sizes) - 1)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        for i, neuron in enumerate(self.neurons[:-1]):\n",
    "            send, _ = neuron.neuron_fn(x if i == 0 else pre)\n",
    "            pre = self.weights[i](send)\n",
    "\n",
    "        final_output, _ = self.neurons[-1].neuron_fn(pre)\n",
    "\n",
    "        # Since we're predicting a single value, we reshape the output to [batch_size, 1]\n",
    "        final_output = final_output.view(batch_size, -1)\n",
    "\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa2783cafc034bb7828a41694e474265",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/5:   0%|          | 0/24 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.3925, Validation Loss: 0.2892\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b872767830624cde8ba11eff736a0494",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/5:   0%|          | 0/24 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1542, Validation Loss: 0.0373\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "054f3677ffc246fea615b7eb811fa6cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/5:   0%|          | 0/24 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0322, Validation Loss: 0.0310\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae524d7fe89b48d48215c568e5dff4b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/5:   0%|          | 0/24 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0299, Validation Loss: 0.0291\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51586f02456849abb925afa526c99782",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/5:   0%|          | 0/24 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0294, Validation Loss: 0.0290\n"
     ]
    }
   ],
   "source": [
    "sequential_stateful_neuron_model = SequentialNeuralDiverseNet(SUNSPOT_LAYER_SIZES).to(DEVICE)\n",
    "seq_criterion = nn.MSELoss()\n",
    "seq_optimizer = torch.optim.Adam(sequential_stateful_neuron_model.parameters(), lr=0.001)\n",
    "\n",
    "train_time_series(model=sequential_stateful_neuron_model, device=DEVICE, train_loader=sunspot_train_loader, \n",
    "      val_loader=sunspot_test_loader, criterion=seq_criterion, opt=seq_optimizer, epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 3: Train Transformer On Shakespeare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define helper function to train the transformer on reconstructing the Shakespeare dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to mask the target tokens\n",
    "# TODO: Using a mask breaks the training process due to a shape error. Needs to be fixed\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = torch.triu(torch.ones(size, size), diagonal=1)\n",
    "    return mask.masked_fill(mask == 1, float('-inf')).masked_fill(mask == 0, float(0.0))\n",
    "\n",
    "def train_shakespeare_trainsformer(model, context_window, step_size, data_loader, optimizer, num_epochs, device=DEVICE, mask=False):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        progress_bar = tqdm(data_loader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=False)\n",
    "\n",
    "        for chunk in progress_bar:\n",
    "            for i in range(0, chunk.size(1) - context_window, step_size):\n",
    "\n",
    "                # Create input and target sequences\n",
    "                input_seq = chunk[:, i:i+context_window].to(device)\n",
    "                target_seq = chunk[:, i+1:i+context_window+1].to(device)\n",
    "\n",
    "                # Optionally create a mask for the target sequence\n",
    "                if mask == True:\n",
    "                    target_seq_mask = create_look_ahead_mask(target_seq.size(1)).to(device)\n",
    "                    target_seq_mask = target_seq_mask.unsqueeze(0)\n",
    "                else:\n",
    "                    target_seq_mask = None\n",
    "\n",
    "                # Zero the gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(input_seq, target_seq, tgt_mask=target_seq_mask)\n",
    "                outputs = outputs.view(-1, outputs.size(-1))\n",
    "                target_seq = target_seq.view(-1)\n",
    "\n",
    "                # Calculate loss and backpropagate\n",
    "                loss = nn.CrossEntropyLoss()(outputs, target_seq)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Logging the loss and update progress bar\n",
    "                epoch_loss += loss.item()\n",
    "                progress_bar.set_postfix(loss=epoch_loss/(i+1))\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} completed. Loss: {epoch_loss/len(data_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Shakespeare dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1120175 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# Define tokenizer used to convert text to tokens\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Create dataset class instance and \n",
    "dataset = ShakespeareDataset(csv_file='data/shakespeare/shakespeare_data.csv', chunk_length=2048, tokenizer=tokenizer)\n",
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the transformer model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the context window size k (defaulting to chunk_length / 2)\n",
    "context_window = 1024\n",
    "\n",
    "# Define the model\n",
    "transformer_model = TransformerModel(vocab_size=tokenizer.vocab_size, max_seq_length=context_window).to(DEVICE)\n",
    "\n",
    "# Define optimizer\n",
    "transformer_optimizer = torch.optim.Adam(transformer_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b70952fc7b74e24a498c6eb69ef48c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/5:   0%|          | 0/546 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 completed. Loss: 108.20048586876838\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48f4bfa2fad24eb087a9f4bb5a954480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/5:   0%|          | 0/546 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 completed. Loss: 107.74317261238238\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49bc4640210448c7bd90e118836e15fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/5:   0%|          | 0/546 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 completed. Loss: 107.73876081892858\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65a78c5397f241f3abf82850d2e92052",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/5:   0%|          | 0/546 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 completed. Loss: 107.70513488696172\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d93e6d6c4e3d4dd885715cb343602219",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/5:   0%|          | 0/546 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 completed. Loss: 107.69152781378219\n"
     ]
    }
   ],
   "source": [
    "# Define the step size to use for the sliding window\n",
    "step_size = 64\n",
    "\n",
    "# Train the model\n",
    "train_shakespeare_trainsformer(transformer_model, context_window, step_size, data_loader, \n",
    "                               optimizer=transformer_optimizer, num_epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
