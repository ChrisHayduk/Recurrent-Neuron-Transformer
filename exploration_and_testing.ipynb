{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stateful Neuron Exploration\n",
    "\n",
    "This notebook will test the performance of the stateful neural network PyTorch port, to verify that the method is \n",
    "implemented correctly and behaves as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import copy\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Imports for the tokenizer, the dataset, and the model\n",
    "from transformers import GPT2Tokenizer\n",
    "from utils.datasets import TextDataLoader\n",
    "from models.transformer_model import TransformerModel\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# MNIST dataset hyperparameters\n",
    "MNIST_INPUT_SIZE = 784\n",
    "MNIST_LAYER_SIZES = [MNIST_INPUT_SIZE, 128, 64, 10]\n",
    "MNIST_BATCH_SIZE = 1000\n",
    "\n",
    "# Sunspot dataset hyperparameters\n",
    "SUNSPOT_INPUT_SIZE = 12\n",
    "SUNSPOT_LAYER_SIZES = [SUNSPOT_INPUT_SIZE, 128, 64, 1]\n",
    "SUNSPOT_BATCH_SIZE = 100\n",
    "\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "# Device configuration\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device('mps')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Set notebook to reload external python modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 1: Stateful Neuron vs. FCN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Dataset Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training set and loader\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "trainset = datasets.MNIST(root='data/test/', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=MNIST_BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Create test set and loader\n",
    "testset = datasets.MNIST(root='data/test/', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=MNIST_BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function: Train Network on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_data, test_data, criterion, opt, epochs):\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Training phase\n",
    "        model.train()  # Set the model to training mode\n",
    "        train_loss = 0.0\n",
    "        for inputs, labels in tqdm(train_data, desc=f'Training: Epoch {epoch+1}/{epochs}', unit='batch'):\n",
    "            # Move the data to the device\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Flatten the images\n",
    "            inputs = inputs.view(inputs.shape[0], -1)  # Flatten the images\n",
    "\n",
    "            # Zero the gradients\n",
    "            opt.zero_grad()\n",
    "\n",
    "            # Forward pass and loss calculation\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and weight update\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            # Logging the loss\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Testing phase\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        test_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_data:\n",
    "                # Move the data to the device\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Flatten the images\n",
    "                inputs = inputs.view(inputs.shape[0], -1)\n",
    "\n",
    "                # Forward pass and loss calculation\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Logging the loss and updating variables at batch level\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Logging the losses\n",
    "        train_loss /= len(trainloader)\n",
    "        test_loss /= len(testloader)\n",
    "        test_accuracy = 100 * correct / total\n",
    "        print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `StatefulNeuronNetwork`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neurons(nn.Module):\n",
    "    def __init__(self, n_neurons):\n",
    "        super(Neurons, self).__init__()\n",
    "\n",
    "        # Initialize matrix neuron parameters and number of neurons to create\n",
    "        self.n_neurons = n_neurons\n",
    "        self.params = nn.Parameter(torch.rand(n_neurons, 3, 3) * 2 - 1)\n",
    "\n",
    "        # Initialize hidden state for batch processing\n",
    "        self.hidden = nn.Parameter(torch.zeros(1, n_neurons, 1), requires_grad=False)\n",
    "    \n",
    "    def neuron_fn(self, inputs):\n",
    "        batch_size = inputs.shape[0]\n",
    "\n",
    "        # Expand hidden to match batch size\n",
    "        hidden_batch = self.hidden.expand(batch_size, -1, -1)\n",
    "\n",
    "        # Ensure inputs is 2D: (batch_size, n_neurons)\n",
    "        inputs = inputs.view(batch_size, -1, 1)\n",
    "        ones = torch.ones_like(inputs)\n",
    "\n",
    "        # Concatenate along the second dimension\n",
    "        stacked = torch.cat((inputs, hidden_batch, ones), dim=1)\n",
    "\n",
    "        # Reshape stacked for matrix multiplication: [batch_size, n_neurons, 3]\n",
    "        stacked = stacked.view(batch_size, self.n_neurons, 3)\n",
    "\n",
    "        # Perform matrix multiplication\n",
    "        dot = torch.tanh(torch.matmul(self.params, stacked.unsqueeze(3)).squeeze(3))\n",
    "\n",
    "        # Update hidden state\n",
    "        self.hidden = nn.Parameter(dot[:, :, -1].unsqueeze(2).detach(), requires_grad=False)\n",
    "\n",
    "        return dot[:, :, 0], dot\n",
    "\n",
    "class NeuralDiverseNet(nn.Module):\n",
    "    def __init__(self, sizes):\n",
    "        super(NeuralDiverseNet, self).__init__()\n",
    "        self.neurons = nn.ModuleList([Neurons(size) for size in sizes])\n",
    "        self.weights = nn.ModuleList([nn.Linear(sizes[i], sizes[i + 1]) for i in range(len(sizes) - 1)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        for i, neuron in enumerate(self.neurons[:-1]):  # Process through all but last layer\n",
    "            send, _ = neuron.neuron_fn(x if i == 0 else pre)\n",
    "            pre = self.weights[i](send)\n",
    "\n",
    "        # Process the last layer\n",
    "        final_output, _ = self.neurons[-1].neuron_fn(pre)\n",
    "\n",
    "        # Reshape the output to ensure it has the shape [batch_size, n_classes]\n",
    "        final_output = final_output.view(batch_size, -1)\n",
    "\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stateful_neuron_model = NeuralDiverseNet(MNIST_LAYER_SIZES).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(stateful_neuron_model.parameters(), lr=0.001)\n",
    "\n",
    "train(model=stateful_neuron_model, device=DEVICE, train_data=trainloader, test_data=testloader, \n",
    "      criterion=criterion, opt=optimizer, epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `FeedforwardNetwork`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, MNIST_LAYER_SIZES):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(len(MNIST_LAYER_SIZES) - 1):\n",
    "            self.layers.append(nn.Linear(MNIST_LAYER_SIZES[i], MNIST_LAYER_SIZES[i + 1]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = torch.relu(layer(x))\n",
    "        x = self.layers[-1](x)  # No activation after the last layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "ff_model = FeedForwardNetwork(MNIST_LAYER_SIZES).to(DEVICE)\n",
    "ff_optimizer = torch.optim.Adam(ff_model.parameters(), lr=0.001)\n",
    "ff_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "train(model=ff_model, device=DEVICE, train_data=trainloader, test_data=testloader, \n",
    "      criterion=ff_criterion, opt=ff_optimizer, epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `SpikingNeuralNetwork`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def surrogate_gradient(x):\n",
    "    alpha = 10  # The steepness of the surrogate gradient\n",
    "    return torch.sigmoid(alpha * x)\n",
    "\n",
    "class SpikingNeuronLayer(nn.Module):\n",
    "    def __init__(self, size_in, size_out, device):\n",
    "        super(SpikingNeuronLayer, self).__init__()\n",
    "        self.device = device\n",
    "        self.synaptic_weights = nn.Parameter(torch.randn(size_in, size_out, device=device) * 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(self.device)\n",
    "        pre_synaptic = torch.matmul(x, self.synaptic_weights)\n",
    "        post_synaptic = surrogate_gradient(pre_synaptic - 1)\n",
    "        return post_synaptic\n",
    "\n",
    "class SpikingNeuralNetwork(nn.Module):\n",
    "    def __init__(self, MNIST_LAYER_SIZES, device):\n",
    "        super(SpikingNeuralNetwork, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.device = device\n",
    "        for i in range(len(MNIST_LAYER_SIZES) - 1):\n",
    "            self.layers.append(SpikingNeuronLayer(MNIST_LAYER_SIZES[i], MNIST_LAYER_SIZES[i + 1], device))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(self.device)  # Ensure input tensor is on the correct device\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the network\n",
    "snn_model = SpikingNeuralNetwork([784, 128, 64, 10], device=DEVICE).to(DEVICE)\n",
    "snn_optimizer = torch.optim.Adam(snn_model.parameters(), lr=0.001)\n",
    "snn_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "train(model=snn_model, device=DEVICE, train_data=trainloader, test_data=testloader, \n",
    "      criterion=snn_criterion, opt=snn_optimizer, epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 2: Stateful Neurons vs. RNN\n",
    "Do stateful neurons perform similarly to recurrent neural networks on a simple time series task?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('data/test/Sunspots.csv', usecols=['Monthly Mean Total Sunspot Number'])\n",
    "data = df.values.astype(float)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data_normalized = scaler.fit_transform(data)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "data_normalized = torch.FloatTensor(data_normalized).view(-1)\n",
    "\n",
    "# Create sequences and corresponding labels\n",
    "sequence_length = 12  # For example, use 12 months to predict the next month\n",
    "sequences = []\n",
    "labels = []\n",
    "\n",
    "for i in range(len(data_normalized) - sequence_length):\n",
    "    sequences.append(data_normalized[i:i+sequence_length])\n",
    "    labels.append(data_normalized[i+sequence_length])\n",
    "\n",
    "sequences = torch.stack(sequences[:-1])\n",
    "labels = torch.stack(labels[1:])\n",
    "\n",
    "# Split the data into training, validation, and testing sets\n",
    "train_sequences, test_sequences, train_labels, test_labels = train_test_split(\n",
    "    sequences, labels, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "# Trim the dataset to an easily divisible length\n",
    "train_sequences = train_sequences[:2400]\n",
    "train_labels = train_labels[:2400]\n",
    "test_sequences = test_sequences[:800]\n",
    "test_labels = test_labels[:800]\n",
    "\n",
    "# Create DataLoaders for each set\n",
    "sunspot_train_loader = DataLoader(TensorDataset(train_sequences, train_labels), shuffle=True, \n",
    "                                  batch_size=SUNSPOT_BATCH_SIZE)\n",
    "sunspot_test_loader = DataLoader(TensorDataset(test_sequences, test_labels), shuffle=False, \n",
    "                                 batch_size=SUNSPOT_BATCH_SIZE)\n",
    "\n",
    "# Print the length of each set\n",
    "print(f'Training set length: {len(train_sequences)}')\n",
    "print(f'Testing set length: {len(test_sequences)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function: Train Network on Sunspot Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_time_series(model, train_loader, val_loader, criterion, opt, epochs, device=DEVICE, model_type=None):\n",
    "    # Perform training\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for sequences, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}', unit='batch'):\n",
    "\n",
    "            # Move the data to the device and reshape the targets\n",
    "            sequences, labels = sequences.to(device), labels.to(device)\n",
    "            sequences = sequences.view(sequences.shape[0], SUNSPOT_INPUT_SIZE, 1).to(device)\n",
    "            labels = labels.unsqueeze(1).to(device)  # Reshape targets\n",
    "\n",
    "            # Zero the gradients\n",
    "            opt.zero_grad()\n",
    "\n",
    "            # Forward pass and loss calculation\n",
    "            outputs = model(sequences)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and weight update\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            # Logging the loss\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for sequences, labels in val_loader:\n",
    "                # Move the data to the device and reshape the targets\n",
    "                sequences, labels = sequences.to(device), labels.to(device)\n",
    "                sequences = sequences.view(sequences.shape[0], SUNSPOT_INPUT_SIZE, 1).to(device)\n",
    "                labels = labels.unsqueeze(1).to(device)  # Reshape targets\n",
    "\n",
    "                # Forward pass and loss calculation\n",
    "                outputs = model(sequences)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f\"Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `RecurrentNeuralNetwork`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaRNN(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=12, output_size=1):\n",
    "        super(VanillaRNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.fc(out[:, -1, :])  # Using the last time step's output\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = VanillaRNN(input_size=1, hidden_size=1, output_size=1).to(DEVICE)\n",
    "rnn_criterion = nn.MSELoss()\n",
    "rnn_optimizer = torch.optim.Adam(rnn_model.parameters(), lr=0.001)\n",
    "\n",
    "train_time_series(model=rnn_model, model_type='RNN', device=DEVICE, train_loader=sunspot_train_loader, \n",
    "      val_loader=sunspot_test_loader, criterion=rnn_criterion, opt=rnn_optimizer, epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `RecurrentStatefulNeuron`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neurons(nn.Module):\n",
    "    def __init__(self, n_neurons):\n",
    "        super(Neurons, self).__init__()\n",
    "\n",
    "        # Initialize matrix neuron parameters and number of neurons to create\n",
    "        self.n_neurons = n_neurons\n",
    "        self.params = nn.Parameter(torch.rand(n_neurons, 3, 3) * 2 - 1)\n",
    "\n",
    "        # Initialize hidden state for batch processing\n",
    "        self.hidden = nn.Parameter(torch.zeros(1, n_neurons, 1), requires_grad=False)\n",
    "    \n",
    "    def neuron_fn(self, inputs):\n",
    "        batch_size = inputs.shape[0]\n",
    "\n",
    "        # Expand hidden to match batch size\n",
    "        hidden_batch = self.hidden.expand(batch_size, -1, -1)\n",
    "\n",
    "        # Ensure inputs is 2D: (batch_size, n_neurons)\n",
    "        inputs = inputs.view(batch_size, -1, 1)\n",
    "        ones = torch.ones_like(inputs)\n",
    "\n",
    "        # Concatenate along the second dimension\n",
    "        stacked = torch.cat((inputs, hidden_batch, ones), dim=1)\n",
    "\n",
    "        # Reshape stacked for matrix multiplication: [batch_size, n_neurons, 3]\n",
    "        stacked = stacked.view(batch_size, self.n_neurons, 3)\n",
    "\n",
    "        # Perform matrix multiplication\n",
    "        dot = torch.tanh(torch.matmul(self.params, stacked.unsqueeze(3)).squeeze(3))\n",
    "\n",
    "        # Update hidden state\n",
    "        self.hidden = nn.Parameter(dot[:, :, -1].unsqueeze(2).detach(), requires_grad=False)\n",
    "\n",
    "        return dot[:, :, 0], dot\n",
    "\n",
    "class SequentialNeuralDiverseNet(nn.Module):\n",
    "    def __init__(self, sizes):\n",
    "        super(SequentialNeuralDiverseNet, self).__init__()\n",
    "        self.neurons = nn.ModuleList([Neurons(size) for size in sizes])\n",
    "        self.weights = nn.ModuleList([nn.Linear(sizes[i], sizes[i + 1]) for i in range(len(sizes) - 1)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        for i, neuron in enumerate(self.neurons[:-1]):\n",
    "            send, _ = neuron.neuron_fn(x if i == 0 else pre)\n",
    "            pre = self.weights[i](send)\n",
    "\n",
    "        final_output, _ = self.neurons[-1].neuron_fn(pre)\n",
    "\n",
    "        # Since we're predicting a single value, we reshape the output to [batch_size, 1]\n",
    "        final_output = final_output.view(batch_size, -1)\n",
    "\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_stateful_neuron_model = SequentialNeuralDiverseNet(SUNSPOT_LAYER_SIZES).to(DEVICE)\n",
    "seq_criterion = nn.MSELoss()\n",
    "seq_optimizer = torch.optim.Adam(sequential_stateful_neuron_model.parameters(), lr=0.001)\n",
    "\n",
    "train_time_series(model=sequential_stateful_neuron_model, device=DEVICE, train_loader=sunspot_train_loader, \n",
    "      val_loader=sunspot_test_loader, criterion=seq_criterion, opt=seq_optimizer, epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 3: Train Transformer On Shakespeare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define helper function to train the transformer on reconstructing the Shakespeare dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to mask the target tokens\n",
    "# TODO: Using a mask breaks the training process due to a shape error. Needs to be fixed\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = torch.triu(torch.ones(size, size), diagonal=1)\n",
    "    return mask.masked_fill(mask == 1, float('-inf')).masked_fill(mask == 0, float(0.0))\n",
    "\n",
    "def train_shakespeare_transformer(model, data_loader, optimizer, num_epochs, device=DEVICE, mask=False):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for inputs, labels in tqdm(data_loader, desc=f'Training: Epoch {epoch+1}/{num_epochs}', unit='batch'):\n",
    "            # Move the data to the device\n",
    "            input_seq = inputs.to(device)\n",
    "            target_seq = labels.to(device)\n",
    "            \n",
    "            # Optionally create a mask for the target sequence\n",
    "            if mask == True:\n",
    "                target_seq_mask = create_look_ahead_mask(target_seq.size(1)).to(device)\n",
    "                target_seq_mask = target_seq_mask.unsqueeze(0)\n",
    "            else:\n",
    "                target_seq_mask = None\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_seq, target_seq, tgt_mask=target_seq_mask)\n",
    "            outputs = outputs.view(-1, outputs.size(-1))\n",
    "            target_seq = target_seq.view(-1)\n",
    "\n",
    "            # Calculate loss and backpropagate\n",
    "            loss = nn.CrossEntropyLoss()(outputs, target_seq)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Logging the loss and update progress bar\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} completed. Loss: {epoch_loss/len(data_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Shakespeare dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset parameters\n",
    "seq_length = 512\n",
    "batch_size = 5\n",
    "file_path = os.path.join(os.getcwd(), 'data/shakespeare', 'tinyshakespeare_100_lines.txt')\n",
    "bpe_tokenizer = 'gpt2'\n",
    "vocab_size = 50257\n",
    "\n",
    "# Create the data loader\n",
    "data_loader = TextDataLoader(file_path, seq_length, bpe_tokenizer, batch_size, vocab_size)\n",
    "train_loader, test_loader = data_loader.create_loaders()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the transformer model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the context window size k (defaulting to chunk_length / 2)\n",
    "context_window = 256\n",
    "\n",
    "# Define the model\n",
    "transformer_model = TransformerModel(vocab_size=vocab_size, max_seq_length=context_window).to(DEVICE)\n",
    "\n",
    "# Define optimizer\n",
    "transformer_optimizer = torch.optim.Adam(transformer_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the step size to use for the sliding window\n",
    "step_size = 16\n",
    "\n",
    "# Train the model\n",
    "train_shakespeare_transformer(transformer_model, train_loader, optimizer=transformer_optimizer, num_epochs=NUM_EPOCHS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
